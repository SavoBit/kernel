
//__wait_on_bit 어감에서, infinite하게 동작할 것이다.
// q->key.flags값이 변경에 의해서 루프 탈출

212 int __sched 
213 __wait_on_bit(wait_queue_head_t *wq, struct wait_bit_queue *q,
214                         int (*action)(void *), unsigned mode)
215 {
216         int ret = 0;
217 
218         do {
219                 prepare_to_wait(wq, &q->wait, mode);
220                 if (test_bit(q->key.bit_nr, q->key.flags))
221                         ret = (*action)(q->key.flags);
222         } while (test_bit(q->key.bit_nr, q->key.flags) && !ret);
223         finish_wait(wq, &q->wait);
224         return ret;
225 }
226 EXPORT_SYMBOL(__wait_on_bit);

# write back? 이란
1. 캐시 정책

# Copy on Write
Process A, fork() Process B 메모리 갱신이 일어나기 전까지는 A의 메모리를 공유한다.

 27 struct anon_vma {
 28     struct anon_vma *root;      /* Root of this anon_vma tree */
 29     struct rw_semaphore rwsem;  /* W: modification, R: walking the list */
 30     /* 
 31      * The refcount is taken on an anon_vma when there is no
 32      * guarantee that the vma of page tables will exist for
 33      * the duration of the operation. A caller that takes
 34      * the reference is responsible for clearing up the
 35      * anon_vma if they are the last user on release
 36      */
 37     atomic_t refcount;
 38 
 39     /* 
 40      * NOTE: the LSB of the rb_root.rb_node is set by
 41      * mm_take_all_locks() _after_ taking the above lock. So the
 42      * rb_root must only be read/written after taking the above lock
 43      * to be sure to see a valid next pointer. The LSB bit itself
 44      * is serialized by a system wide lock only visible to
 45      * mm_take_all_locks() (mm_all_locks_mutex).
 46      */
 47     struct rb_root rb_root; /* Interval tree of private "related" vmas */
 48 };


 41 struct page {
 42     /* First double word block */
 43     unsigned long flags;        /* Atomic flags, some possibly
 44                      * updated asynchronously */
 45     struct address_space *mapping;  /* If low bit clear, points to
 46                      * inode address_space, or NULL.
 47                      * If page mapped as anonymous
 48                      * memory, low bit is set, and
 49                      * it points to anon_vma object:
 50                      * see PAGE_MAPPING_ANON below.
 51                      */

 410 // 2015-07-04;
 411 struct address_space {
 412     struct inode        *host;      /* owner: inode, block_device */
 413     struct radix_tree_root  page_tree;  /* radix tree of all pages */
 414     spinlock_t      tree_lock;  /* and lock protecting it */
 415     unsigned int        i_mmap_writable;/* count VM_SHARED mappings */
 416     struct rb_root      i_mmap;     /* tree of private and shared mappings */
 417     struct list_head    i_mmap_nonlinear;/*list VM_NONLINEAR mappings */
 418     struct mutex        i_mmap_mutex;   /* protect tree, count, list */
 419     /* Protected by tree_lock together with the radix tree */
 420     unsigned long       nrpages;    /* number of total pages */
 421     pgoff_t         writeback_index;/* writeback starts here */
 422     const struct address_space_operations *a_ops;   /* methods */
 423     unsigned long       flags;      /* error bits/gfp mask */
 424     struct backing_dev_info *backing_dev_info; /* device readahead, etc */
 425     spinlock_t      private_lock;   /* for use by the address_space */
 426     struct list_head    private_list;   /* ditto */
 427     void            *private_data;  /* ditto */
 428 } __attribute__((aligned(sizeof(long))));

ANON이면 +1시켜놓는다. low bit LSB

rwsem_is_locked

GCC의 이런 문법
251 #define raw_spin_trylock_irqsave(lock, flags) \
252 ({ \
253     local_irq_save(flags); \
254     raw_spin_trylock(lock) ? \
255     1 : ({ local_irq_restore(flags); 0; }); \
256 })


 16 /*      
 17  * the rw-semaphore definition
 18  * - if activity is 0 then there are no active readers or writers
 19  * - if activity is +ve then that is the number of active readers
 20  * - if activity is -1 then there is one active writer
 21  * - if wait_list is not empty, then there are processes waiting for the semaphore
 22  */
 23 struct rw_semaphore {
 24     __s32           activity;
 25     raw_spinlock_t      wait_lock;
 26     struct list_head    wait_list;
 27 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 28     struct lockdep_map dep_map;
 29 #endif  
 30 }; 


 42 /*
 43  * lock for writing
 44  */
 45 void __sched down_write(struct rw_semaphore *sem)
 46 {
 47         might_sleep();
 48         rwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);
 49 
 50         LOCK_CONTENDED(sem, __down_write_trylock, __down_write);
 51 }

rwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);은 뭔가?

rwsem-spinlock.c를 사용한다.
lib/Makefile
lib-$(CONFIG_RWSEM_GENERIC_SPINLOCK) += rwsem-spinlock.o
lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o

include/linux/rwsem.h
 21 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK    // set
 22 #include <linux/rwsem-spinlock.h> /* use a generic implementation */
 23 #else
 24 /* All arch specific implementations share the same struct */
 25 struct rw_semaphore {
 26     long            count;
 27     raw_spinlock_t      wait_lock;
 28     struct list_head    wait_list;
 29 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 30     struct lockdep_map  dep_map;
 31 #endif
 32 };
 33 
 34 extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
 35 extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
 36 extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *);
 37 extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
 38 
 39 /* Include the arch specific part */
 40 #include <asm/rwsem.h>
 41 
 42 /* In all implementations count != 0 means locked */
 43 static inline int rwsem_is_locked(struct rw_semaphore *sem)
 44 {
 45     return sem->count != 0;
 46 }
 47 
 48 #endif





